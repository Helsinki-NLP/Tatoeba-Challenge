# opus-2021-01-03.zip

* dataset: opus
* model: transformer
* source language(s): fra
* target language(s): deu
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* download: [opus-2021-01-03.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/fra-deu/opus-2021-01-03.zip)
* test set translations: [opus-2021-01-03.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/fra-deu/opus-2021-01-03.test.txt)
* test set scores: [opus-2021-01-03.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/fra-deu/opus-2021-01-03.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| euelections_dev2019.fr-de-fradeu.fra.deu 	| 26.4 	| 0.572 |
| newssyscomb2009-fradeu.fra.deu 	| 22.5 	| 0.528 |
| news-test2008-fradeu.fra.deu 	| 22.2 	| 0.527 |
| newstest2009-fradeu.fra.deu 	| 22.2 	| 0.525 |
| newstest2010-fradeu.fra.deu 	| 23.1 	| 0.532 |
| newstest2011-fradeu.fra.deu 	| 21.9 	| 0.522 |
| newstest2012-fradeu.fra.deu 	| 22.9 	| 0.520 |
| newstest2013-fradeu.fra.deu 	| 24.7 	| 0.537 |
| newstest2019-frde-fradeu.fra.deu 	| 27.6 	| 0.595 |
| Tatoeba-test.fra.deu 	| 49.5 	| 0.678 |
| Tatoeba-test.fra-deu.fra.deu 	| 49.5 	| 0.678 |

