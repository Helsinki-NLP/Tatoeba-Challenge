# opus-2020-07-20.zip

* dataset: opus
* model: transformer
* source language(s): apc ara arq arz heb mlt
* target language(s): apc ara arq arz heb mlt
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)
* download: [opus-2020-07-20.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-07-20.zip)
* test set translations: [opus-2020-07-20.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-07-20.test.txt)
* test set scores: [opus-2020-07-20.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-07-20.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba-test.multi.multi 	| 27.1 	| 0.507 |

# opus-2020-07-27.zip

* dataset: opus
* model: transformer
* source language(s): apc ara arq arz heb mlt
* target language(s): apc ara arq arz heb mlt
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)
* download: [opus-2020-07-27.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-07-27.zip)
* test set translations: [opus-2020-07-27.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-07-27.test.txt)
* test set scores: [opus-2020-07-27.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-07-27.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba-test.ara-ara.ara.ara 	| 4.2 	| 0.200 |
| Tatoeba-test.ara-heb.ara.heb 	| 34.0 	| 0.542 |
| Tatoeba-test.ara-mlt.ara.mlt 	| 16.6 	| 0.513 |
| Tatoeba-test.heb-ara.heb.ara 	| 18.8 	| 0.477 |
| Tatoeba-test.mlt-ara.mlt.ara 	| 20.7 	| 0.388 |
| Tatoeba-test.multi.multi 	| 27.1 	| 0.507 |

# opus-2020-09-26.zip

* dataset: opus
* model: transformer
* source language(s): acm afb amh apc ara arq ary arz eng heb mlt phn_Phnx syc_Syrc tir tmr_Hebr
* target language(s): acm afb amh apc ara arq ary arz eng heb mlt phn_Phnx syc_Syrc tir tmr_Hebr
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)
* download: [opus-2020-09-26.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-09-26.zip)
* test set translations: [opus-2020-09-26.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-09-26.test.txt)
* test set scores: [opus-2020-09-26.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sem-sem/opus-2020-09-26.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba-test.amh-eng.amh.eng 	| 42.0 	| 0.593 |
| Tatoeba-test.ara-ara.ara.ara 	| 2.7 	| 0.167 |
| Tatoeba-test.ara-eng.ara.eng 	| 38.6 	| 0.564 |
| Tatoeba-test.ara-heb.ara.heb 	| 34.9 	| 0.558 |
| Tatoeba-test.ara-mlt.ara.mlt 	| 24.3 	| 0.532 |
| Tatoeba-test.ara-tmr.ara.tmr 	| 2.7 	| 0.014 |
| Tatoeba-test.eng-amh.eng.amh 	| 13.7 	| 0.510 |
| Tatoeba-test.eng-ara.eng.ara 	| 12.2 	| 0.412 |
| Tatoeba-test.eng-heb.eng.heb 	| 32.1 	| 0.550 |
| Tatoeba-test.eng-mlt.eng.mlt 	| 17.6 	| 0.556 |
| Tatoeba-test.eng-phn.eng.phn 	| 1.3 	| 0.007 |
| Tatoeba-test.eng-tir.eng.tir 	| 2.6 	| 0.250 |
| Tatoeba-test.eng-tmr.eng.tmr 	| 1.1 	| 0.007 |
| Tatoeba-test.heb-ara.heb.ara 	| 19.5 	| 0.496 |
| Tatoeba-test.heb-eng.heb.eng 	| 43.3 	| 0.598 |
| Tatoeba-test.heb-phn.heb.phn 	| 2.0 	| 0.009 |
| Tatoeba-test.heb-syc.heb.syc 	| 3.3 	| 0.000 |
| Tatoeba-test.heb-tmr.heb.tmr 	| 0.2 	| 0.005 |
| Tatoeba-test.mlt-ara.mlt.ara 	| 17.3 	| 0.427 |
| Tatoeba-test.mlt-eng.mlt.eng 	| 48.3 	| 0.647 |
| Tatoeba-test.multi.multi 	| 33.2 	| 0.534 |
| Tatoeba-test.phn-eng.phn.eng 	| 2.2 	| 0.071 |
| Tatoeba-test.phn-heb.phn.heb 	| 0.4 	| 0.044 |
| Tatoeba-test.phn-tmr.phn.tmr 	| 0.5 	| 0.000 |
| Tatoeba-test.syc-heb.syc.heb 	| 0.0 	| 0.000 |
| Tatoeba-test.tir-eng.tir.eng 	| 16.1 	| 0.344 |
| Tatoeba-test.tmr-ara.tmr.ara 	| 2.5 	| 0.075 |
| Tatoeba-test.tmr-eng.tmr.eng 	| 2.2 	| 0.141 |
| Tatoeba-test.tmr-heb.tmr.heb 	| 1.0 	| 0.142 |
| Tatoeba-test.tmr-phn.tmr.phn 	| 0.0 	| 0.017 |

