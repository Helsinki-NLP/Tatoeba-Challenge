# opus-2020-07-04.zip

* dataset: opus
* model: transformer
* source language(s): bel bel_Latn bul bul_Latn ces csb_Latn dsb hsb mkd orv_Cyrl pol rue rus slv ukr
* target language(s): eng
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* download: [opus-2020-07-04.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus-2020-07-04.zip)
* test set translations: [opus-2020-07-04.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus-2020-07-04.test.txt)
* test set scores: [opus-2020-07-04.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus-2020-07-04.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba-test.bel-eng.bel.eng 	| 39.7 	| 0.586 |
| Tatoeba-test.bul-eng.bul.eng 	| 54.7 	| 0.692 |
| Tatoeba-test.ces-eng.ces.eng 	| 52.6 	| 0.684 |
| Tatoeba-test.csb-eng.csb.eng 	| 17.1 	| 0.382 |
| Tatoeba-test.dsb-eng.dsb.eng 	| 17.3 	| 0.359 |
| Tatoeba-test.hsb-eng.hsb.eng 	| 31.2 	| 0.502 |
| Tatoeba-test.mkd-eng.mkd.eng 	| 52.8 	| 0.665 |
| Tatoeba-test.multi.eng 	| 52.0 	| 0.671 |
| Tatoeba-test.orv-eng.orv.eng 	| 13.0 	| 0.287 |
| Tatoeba-test.pol-eng.pol.eng 	| 49.9 	| 0.663 |
| Tatoeba-test.rue-eng.rue.eng 	| 19.7 	| 0.363 |
| Tatoeba-test.rus-eng.rus.eng 	| 52.8 	| 0.677 |
| Tatoeba-test.slv-eng.slv.eng 	| 41.5 	| 0.595 |
| Tatoeba-test.ukr-eng.ukr.eng 	| 52.1 	| 0.674 |

# opus-2020-07-27.zip

* dataset: opus
* model: transformer
* source language(s): bel bel_Latn bos_Latn bul bul_Latn ces csb_Latn dsb hrv hsb mkd orv_Cyrl pol rue rus slv srp_Cyrl srp_Latn ukr
* target language(s): eng
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* download: [opus-2020-07-27.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus-2020-07-27.zip)
* test set translations: [opus-2020-07-27.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus-2020-07-27.test.txt)
* test set scores: [opus-2020-07-27.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus-2020-07-27.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009-ceseng.ces.eng 	| 25.7 	| 0.536 |
| newstest2009-ceseng.ces.eng 	| 24.7 	| 0.531 |
| newstest2010-ceseng.ces.eng 	| 25.3 	| 0.541 |
| newstest2011-ceseng.ces.eng 	| 26.0 	| 0.538 |
| newstest2012-ceseng.ces.eng 	| 25.0 	| 0.533 |
| newstest2012-ruseng.rus.eng 	| 31.8 	| 0.584 |
| newstest2013-ceseng.ces.eng 	| 28.3 	| 0.553 |
| newstest2013-ruseng.rus.eng 	| 25.9 	| 0.530 |
| newstest2014-csen-ceseng.ces.eng 	| 30.5 	| 0.583 |
| newstest2014-ruen-ruseng.rus.eng 	| 28.9 	| 0.570 |
| newstest2015-encs-ceseng.ces.eng 	| 27.7 	| 0.544 |
| newstest2015-enru-ruseng.rus.eng 	| 27.5 	| 0.547 |
| newstest2016-encs-ceseng.ces.eng 	| 29.3 	| 0.563 |
| newstest2016-enru-ruseng.rus.eng 	| 26.8 	| 0.543 |
| newstest2017-encs-ceseng.ces.eng 	| 26.0 	| 0.534 |
| newstest2017-enru-ruseng.rus.eng 	| 30.2 	| 0.569 |
| newstest2018-encs-ceseng.ces.eng 	| 27.1 	| 0.544 |
| newstest2018-enru-ruseng.rus.eng 	| 25.9 	| 0.539 |
| newstest2019-ruen-ruseng.rus.eng 	| 28.8 	| 0.558 |
| Tatoeba-test.bel-eng.bel.eng 	| 41.7 	| 0.599 |
| Tatoeba-test.bul-eng.bul.eng 	| 55.2 	| 0.696 |
| Tatoeba-test.ces-eng.ces.eng 	| 52.8 	| 0.686 |
| Tatoeba-test.csb-eng.csb.eng 	| 21.4 	| 0.399 |
| Tatoeba-test.dsb-eng.dsb.eng 	| 36.2 	| 0.511 |
| Tatoeba-test.hbs-eng.hbs.eng 	| 55.9 	| 0.701 |
| Tatoeba-test.hsb-eng.hsb.eng 	| 37.7 	| 0.545 |
| Tatoeba-test.mkd-eng.mkd.eng 	| 54.5 	| 0.680 |
| Tatoeba-test.multi.eng 	| 52.8 	| 0.680 |
| Tatoeba-test.orv-eng.orv.eng 	| 12.0 	| 0.296 |
| Tatoeba-test.pol-eng.pol.eng 	| 50.6 	| 0.668 |
| Tatoeba-test.rue-eng.rue.eng 	| 22.6 	| 0.412 |
| Tatoeba-test.rus-eng.rus.eng 	| 53.5 	| 0.682 |
| Tatoeba-test.slv-eng.slv.eng 	| 42.8 	| 0.603 |
| Tatoeba-test.ukr-eng.ukr.eng 	| 53.4 	| 0.683 |

# opus4m-2020-08-12.zip

* dataset: opus4m
* model: transformer
* source language(s): bel bel_Latn bos_Latn bul bul_Latn ces csb_Latn dsb hrv hsb mkd orv_Cyrl pol rue rus slv srp_Cyrl srp_Latn ukr
* target language(s): eng
* model: transformer
* pre-processing: normalization + SentencePiece (spm32k,spm32k)
* download: [opus4m-2020-08-12.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus4m-2020-08-12.zip)
* test set translations: [opus4m-2020-08-12.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus4m-2020-08-12.test.txt)
* test set scores: [opus4m-2020-08-12.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/sla-eng/opus4m-2020-08-12.eval.txt)

## Benchmarks

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009-ceseng.ces.eng 	| 26.4 	| 0.541 |
| newstest2009-ceseng.ces.eng 	| 25.5 	| 0.535 |
| newstest2010-ceseng.ces.eng 	| 26.1 	| 0.547 |
| newstest2011-ceseng.ces.eng 	| 26.7 	| 0.544 |
| newstest2012-ceseng.ces.eng 	| 25.9 	| 0.538 |
| newstest2012-ruseng.rus.eng 	| 32.6 	| 0.589 |
| newstest2013-ceseng.ces.eng 	| 28.8 	| 0.556 |
| newstest2013-ruseng.rus.eng 	| 26.5 	| 0.534 |
| newstest2014-csen-ceseng.ces.eng 	| 31.5 	| 0.591 |
| newstest2014-ruen-ruseng.rus.eng 	| 29.8 	| 0.577 |
| newstest2015-encs-ceseng.ces.eng 	| 28.5 	| 0.549 |
| newstest2015-enru-ruseng.rus.eng 	| 28.2 	| 0.552 |
| newstest2016-encs-ceseng.ces.eng 	| 30.0 	| 0.569 |
| newstest2016-enru-ruseng.rus.eng 	| 27.6 	| 0.550 |
| newstest2017-encs-ceseng.ces.eng 	| 26.6 	| 0.538 |
| newstest2017-enru-ruseng.rus.eng 	| 31.3 	| 0.576 |
| newstest2018-encs-ceseng.ces.eng 	| 27.8 	| 0.548 |
| newstest2018-enru-ruseng.rus.eng 	| 27.0 	| 0.546 |
| newstest2019-ruen-ruseng.rus.eng 	| 29.9 	| 0.566 |
| Tatoeba-test.bel-eng.bel.eng 	| 42.7 	| 0.610 |
| Tatoeba-test.bul-eng.bul.eng 	| 55.2 	| 0.696 |
| Tatoeba-test.ces-eng.ces.eng 	| 53.2 	| 0.690 |
| Tatoeba-test.csb-eng.csb.eng 	| 25.0 	| 0.447 |
| Tatoeba-test.dsb-eng.dsb.eng 	| 30.9 	| 0.463 |
| Tatoeba-test.hbs-eng.hbs.eng 	| 55.8 	| 0.701 |
| Tatoeba-test.hsb-eng.hsb.eng 	| 49.8 	| 0.618 |
| Tatoeba-test.mkd-eng.mkd.eng 	| 54.7 	| 0.682 |
| Tatoeba-test.multi.eng 	| 53.2 	| 0.683 |
| Tatoeba-test.orv-eng.orv.eng 	| 13.8 	| 0.293 |
| Tatoeba-test.pol-eng.pol.eng 	| 51.0 	| 0.671 |
| Tatoeba-test.rue-eng.rue.eng 	| 23.1 	| 0.421 |
| Tatoeba-test.rus-eng.rus.eng 	| 54.1 	| 0.685 |
| Tatoeba-test.slv-eng.slv.eng 	| 44.3 	| 0.621 |
| Tatoeba-test.ukr-eng.ukr.eng 	| 53.6 	| 0.685 |

